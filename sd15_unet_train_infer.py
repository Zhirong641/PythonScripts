# -*- coding: utf-8 -*-
import os, io, csv, math, json, glob, argparse, random, hashlib
from dataclasses import dataclass
from typing import List, Tuple
import numpy as np
from PIL import Image

import torch
import torch.nn.functional as F
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from tqdm import tqdm

from diffusers import UNet2DConditionModel, DDPMScheduler, AutoencoderKL
from transformers import CLIPTokenizer, CLIPTextModel
from safetensors.torch import save_file as safetensors_save, load_file as safetensors_load

from CSVProcessor import CSVProcessor

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import csv
from collections import deque

# -------------------------
# å…¨å±€å¸¸é‡ï¼ˆSD1.xæƒ¯ä¾‹ï¼‰
# -------------------------
VAE_SCALE = 0.18215
MAX_TOKEN = 77
CLIP_MODEL = "openai/clip-vit-large-patch14"
SD15_REPO = "runwayml/stable-diffusion-v1-5"

# # -------------------------
# # å›¾åƒé¢„å¤„ç†ï¼ˆåˆ° 512x512ï¼‰
# # -------------------------
# def make_image_transform(resolution=512):
#     return transforms.Compose([
#         transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BICUBIC),
#         transforms.CenterCrop(resolution),
#         transforms.ToTensor(),  # [0,1]
#         transforms.Normalize([0.5], [0.5])  # -> [-1,1]
#     ])

# å›¾åƒé¢„å¤„ç†ï¼ˆPaddingåˆ° 512x512ï¼‰
def make_image_transform(resolution=512):
    def resize_pad(img: Image.Image) -> Image.Image:
        # ç­‰æ¯”ç¼©æ”¾
        w, h = img.size
        scale = resolution / max(w, h)
        new_w, new_h = int(w * scale), int(h * scale)
        img = img.resize((new_w, new_h), Image.BICUBIC)

        # Padding åˆ° resolutionÃ—resolution
        from PIL import ImageOps
        pad_w = (resolution - new_w) // 2
        pad_h = (resolution - new_h) // 2
        pad = (pad_w, pad_h, resolution - new_w - pad_w, resolution - new_h - pad_h)
        img = ImageOps.expand(img, border=pad, fill=(0, 0, 0))
        return img

    return transforms.Compose([
        transforms.Lambda(resize_pad),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5])  # [0,1] â†’ [-1,1]
    ])

def save_tensor_image(img_t: torch.Tensor, path: str):
    """
    img_t: [B,3,H,W] in [-1, 1]
    """
    img = (img_t.clamp(-1, 1) * 0.5 + 0.5)  # [-1,1] -> [0,1]
    img = (img * 255.0).round().byte().cpu().permute(0,2,3,1).numpy()  # [B,H,W,3]
    os.makedirs(os.path.dirname(path), exist_ok=True)
    from PIL import Image
    Image.fromarray(img[0]).save(path)

# -------------------------
# å®ç”¨å‡½æ•°
# -------------------------
def seed_everything(seed: int):
    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = True
    try:
        torch.set_float32_matmul_precision('high')
    except Exception:
        pass

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8")).hexdigest()

# -------------------------
# é¢„ç¼–ç å­å‘½ä»¤
# è¾“å…¥ï¼šCSVï¼ˆä¸¤åˆ—ï¼špath,captionï¼‰ï¼Œæˆ–ç»™å‡ºç›®å½•ï¼ˆcaption=ç©ºï¼‰
# è¾“å‡ºï¼šæ¯å¼ å›¾ä¸€ä¸ª .npzï¼ˆlatent(4,64,64,fp16) & text_emb(77,768,fp16)ï¼‰ï¼Œä»¥åŠ uncond_emb.npy
# -------------------------
def cmd_encode(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    os.makedirs(args.out_dir, exist_ok=True)
    tfm = make_image_transform(args.size)

    print(">> loading VAE & CLIPâ€¦")
    vae: AutoencoderKL = AutoencoderKL.from_pretrained(SD15_REPO, subfolder="vae", torch_dtype=torch.float16).to(device)
    vae.requires_grad_(False).eval()
    tokenizer = CLIPTokenizer.from_pretrained(CLIP_MODEL)
    text_encoder = CLIPTextModel.from_pretrained(CLIP_MODEL, torch_dtype=torch.float16).to(device)
    text_encoder.requires_grad_(False).eval()

    # è§£ææ•°æ®æº
    samples = []
    if args.csv:
        # CSV æ–‡ä»¶ï¼›å¿…é¡»åŒ…å« path, caption åˆ—
        processor = CSVProcessor(args.csv)
        data = processor.get_data()
        print("Original rows len:", len(data))
        exclude_word_list = ["no humans", "chibi", "character profile", "lineart",
                             "sketch", "monochrome", "comic", "text focus", "1990s", "1980s",
                             "retro artstyle", "abstract"]
        random.shuffle(data)
        data = [row for row in data if not any(exclude in row[1] for exclude in exclude_word_list)]
        print("Excluding rows len:", len(data))
        if len(data) > 0:
            print("First row: ", data[0])
        for row in data:
            samples.append((row[0], row[1]))
    else:
        # å›¾åƒç›®å½•ï¼›caption ç½®ç©º
        img_ext = {".jpg",".jpeg",".png",".webp",".bmp"}
        for p in glob.glob(os.path.join(args.data_dir, "**", "*"), recursive=True):
            if os.path.splitext(p)[1].lower() in img_ext:
                samples.append((p, ""))

    # é¢„å…ˆè®¡ç®— uncond åµŒå…¥
    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.float16):
        ids_uncond = tokenizer([""], padding="max_length", max_length=MAX_TOKEN, truncation=True, return_tensors="pt").input_ids.to(device)
        uncond_emb = text_encoder(ids_uncond)[0].detach().cpu().to(torch.float16).numpy()
        np.save(os.path.join(args.out_dir, "uncond_emb.npy"), uncond_emb)

    index_path = os.path.join(args.out_dir, "index.jsonl")
    idxf = open(index_path, "w", encoding="utf-8")

    print(f">> encoding {len(samples)} samplesâ€¦")
    for (path, caption) in tqdm(samples):
        try:
            img = Image.open(path).convert("RGB")
        except Exception as e:
            print(f"[skip] open {path}: {e}")
            continue

        pixel = tfm(img).unsqueeze(0).to(device)  # [1,3,512,512]
        with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.float16):
            # VAE -> latent
            lat = vae.encode(pixel).latent_dist.sample() * VAE_SCALE  # [1,4,64,64]
            # æ–‡æœ¬åµŒå…¥
            ids = tokenizer(caption, padding="max_length", max_length=MAX_TOKEN, truncation=True, return_tensors="pt").input_ids.to(device)
            emb = text_encoder(ids)[0]  # [1,77,768]

        lat = lat[0].detach().cpu().to(torch.float16).numpy()
        emb = emb[0].detach().cpu().to(torch.float16).numpy()

        base = sha1(path) + ".npz"
        npz_path = os.path.join(args.out_dir, base)
        # ä¸å†™ caption åˆ° npz
        np.savez_compressed(npz_path, latent=lat, text_emb=emb, src=path)
        # åœ¨ index.jsonl å†™å…¥ npz æ–‡ä»¶åä¸ caption
        idxf.write(json.dumps({"npz": base, "src": path, "caption": caption}, ensure_ascii=False) + "\n")
    idxf.close()
    print(">> done. Saved to", args.out_dir)


def cmd_decode(args):
    """
    å°† latent è§£ç å›å›¾ç‰‡ã€‚
    æ”¯æŒè¾“å…¥ï¼š
      - å•ä¸ª .npzï¼ˆåŒ…å« 'latent' é”®ï¼‰
      - ç›®å½•ï¼šæ‰¹é‡å¯¹å…¶ä¸­æ‰€æœ‰ .npz è¿›è¡Œè§£ç 
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    dtype = torch.float16 if device.type == "cuda" else torch.float32

    # è½½å…¥ VAEï¼ˆSD1.5ï¼‰
    vae: AutoencoderKL = AutoencoderKL.from_pretrained(
        SD15_REPO, subfolder="vae", torch_dtype=dtype
    ).to(device)
    vae.eval().requires_grad_(False)

    def decode_one(npz_path: str, out_path: str):
        z = np.load(npz_path)
        assert "latent" in z, f"{npz_path} ä¸å« 'latent'"
        lat = torch.from_numpy(z["latent"]).to(device=device, dtype=dtype)  # [4,h,w]
        lat = lat.unsqueeze(0)  # [1,4,H,W]
        # è®­ç»ƒ/é‡‡æ ·æ—¶ latent ä¹˜äº† 0.18215ï¼Œè¿™é‡Œè¦é™¤å›å»
        lat = lat / args.scale

        with torch.no_grad(), torch.amp.autocast('cuda', enabled=(dtype==torch.float16), dtype=dtype):
            img = vae.decode(lat).sample  # [-1,1], [1,3,H*8,W*8]ï¼ˆè‹¥H=64åˆ™è¾“å‡º512ï¼‰
        save_tensor_image(img, out_path)
        print(f"decoded -> {out_path}, latent shape: {lat.shape}, image shape: {img.shape}")

    # è¾“å…¥å¯ä»¥æ˜¯æ–‡ä»¶æˆ–ç›®å½•
    if os.path.isdir(args.input):
        files = sorted([p for p in glob.glob(os.path.join(args.input, "*.npz"))])
        assert files, f"{args.input} ä¸‹æ²¡æœ‰ .npz"
        os.makedirs(args.out_dir, exist_ok=True)
        for p in files:
            base = os.path.splitext(os.path.basename(p))[0]
            out = os.path.join(args.out_dir, base + ".png")
            decode_one(p, out)
    else:
        # å•æ–‡ä»¶
        in_path = args.input
        out_path = args.out
        if out_path is None:
            # é»˜è®¤è¾“å‡ºåˆ°åŒç›®å½•åŒå .png
            out_path = os.path.splitext(in_path)[0] + ".png"
        decode_one(in_path, out_path)

# -------------------------
# è®­ç»ƒæ•°æ®é›†ï¼ˆè¯» .npz + ä» index.jsonl å– captionï¼‰
# -------------------------
class LatentCLIPDataset(Dataset):
    """
    ä»…æŠŠ index.jsonl è¯»æˆ (npz_filename, caption) åˆ—è¡¨ï¼Œ__getitem__ æ—¶æŒ‰éœ€è¯»å– .npzã€‚
    è¿™æ ·æ—¢èƒ½çœå†…å­˜ï¼Œä¹Ÿèƒ½åœ¨ä¿å­˜æ—¶è·å–å½“å‰ batch çš„ captionã€‚
    """
    def __init__(self, index_jsonl: str, root_dir: str, return_caption: bool = True):
        self.root = root_dir
        self.return_caption = return_caption
        if not os.path.isfile(index_jsonl):
            raise FileNotFoundError(f"index file not found: {index_jsonl}")
        self.items = []
        with open(index_jsonl, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    j = json.loads(line)
                    fname = j["npz"]
                    cap = j.get("caption", "")
                    full_path = os.path.join(root_dir, fname)
                    if os.path.isfile(full_path):
                        self.items.append((fname, cap))
                    else:
                        print(f"[warn] missing file: {full_path}")
                except Exception:
                    continue

    def __len__(self): return len(self.items)

    def __getitem__(self, i):
        fname, caption = self.items[i]
        p = os.path.join(self.root, fname)
        z = np.load(p, allow_pickle=False)
        lat = z["latent"].astype(np.float16)          # (4,64,64)
        emb = z["text_emb"].astype(np.float16)        # (77,768)
        if self.return_caption:
            return torch.from_numpy(lat), torch.from_numpy(emb), caption
        else:
            return torch.from_numpy(lat), torch.from_numpy(emb)

# -------------------------
# EMA
# -------------------------
class EMA:
    def __init__(self, model: nn.Module, decay: float = 0.999):
        self.decay = decay
        self.shadow = {}
        self.collected_params = []
        for n, p in model.named_parameters():
            if p.requires_grad:
                self.shadow[n] = p.data.detach().clone()
    @torch.no_grad()
    def update(self, model: nn.Module):
        for n, p in model.named_parameters():
            if not p.requires_grad:
                continue
            assert n in self.shadow
            self.shadow[n].mul_(self.decay).add_(p.data, alpha=1.0 - self.decay)
    def copy_to(self, model: nn.Module):
        for n, p in model.named_parameters():
            if n in self.shadow:
                p.data.copy_(self.shadow[n])
    def state_dict(self):
        # åªå­˜ shadow æƒé‡ï¼Œä½“ç§¯å°ã€æ¢å¤ç®€å•
        return {"decay": self.decay, "shadow": {k: v.cpu() for k, v in self.shadow.items()}}

    def load_state_dict(self, state):
        self.decay = state["decay"]
        sh = state["shadow"]
        # ä¿è¯ key å®Œæ•´
        for n, p in self.shadow.items():
            if n in sh:
                self.shadow[n] = sh[n].to(p.device, dtype=p.dtype)

def save_train_state(path, unet, optimizer, lr_sched, ema: EMA, global_step, epoch, prediction_type, scaler=None, opt_step=0):
    pkg = {
        "model": {k: v.detach().cpu() for k, v in unet.state_dict().items()},
        "optimizer": optimizer.state_dict(),
        "lr_sched_step": lr_sched.step_idx if hasattr(lr_sched, "step_idx") else 0,
        "ema": ema.state_dict(),
        "global_step": global_step,
        "epoch": epoch,
        "prediction_type": prediction_type,
        "opt_step": opt_step,
        "scaler": (scaler.state_dict() if (scaler is not None and hasattr(scaler, "state_dict")) else None),
    }
    os.makedirs(os.path.dirname(path), exist_ok=True
    )
    torch.save(pkg, path)
    print(f">> saved train state: {path}")

def try_load_train_state(resume_path, unet, optimizer, lr_sched, ema: EMA, scaler=None):
    """
    æ”¯æŒä¸‰ç§è¾“å…¥ï¼š
      1) è®­ç»ƒçŠ¶æ€åŒ… .pth/.ptï¼ˆæ¨èï¼‰
      2) ç›®å½•ï¼šè‡ªåŠ¨æ‰¾å…¶ä¸­çš„ *.pthï¼ˆæŒ‰æ–‡ä»¶åæ’åºå–æœ€åä¸€ä¸ªï¼‰
      3) ä»… raw æ¨¡å‹æƒé‡ .ptï¼ˆåªèƒ½æ¢å¤æ¨¡å‹ï¼‰
    """
    def _load_pkg(p):
        print(f">> resume from: {p}")
        pkg = torch.load(p, map_location="cpu")
        unet.load_state_dict(pkg["model"], strict=True)
        # optimizer / lr / ema / scaler å¦‚æœå­˜åœ¨å°±æ¢å¤
        if "optimizer" in pkg and len(pkg["optimizer"]) > 0:
            optimizer.load_state_dict(pkg["optimizer"])
        if "lr_sched_step" in pkg:
            lr_sched.step_idx = pkg["lr_sched_step"]
        if "ema" in pkg:
            ema.load_state_dict(pkg["ema"])
        if scaler is not None and pkg.get("scaler") is not None:
            try:
                scaler.load_state_dict(pkg["scaler"])
            except Exception as e:
                print(f"[warn] scaler state not restored: {e}")
        return int(pkg.get("global_step", 0)), int(pkg.get("epoch", 0)), pkg.get("prediction_type", "epsilon"), int(pkg.get("opt_step", 0))

    if resume_path is None:
        return 0, 0, None, 0

    if os.path.isdir(resume_path):
        cands = [os.path.join(resume_path, x) for x in os.listdir(resume_path) if x.endswith(".pth") or x.endswith(".pt")]
        if not cands:
            raise FileNotFoundError(f"No *.pth found in {resume_path}")
        cands.sort()
        return _load_pkg(cands[-1])

    # æ–‡ä»¶
    if resume_path.endswith(".pth") or resume_path.endswith(".pt"):
        # å°è¯•æŒ‰â€œå®Œæ•´è®­ç»ƒåŒ…â€åŠ è½½
        try:
            return _load_pkg(resume_path)
        except Exception:
            # é€€åŒ–ä¸ºâ€œä»…æ¨¡å‹æƒé‡â€
            print(">> resume file is raw model only; loading UNet weights")
            sd = torch.load(resume_path, map_location="cpu")
            unet.load_state_dict(sd, strict=False)
            return 0, 0, None, 0

    raise FileNotFoundError(resume_path)

# -------------------------
# Min-SNR æƒé‡
# weight = min(snr, gamma) / (snr + 1)
# -------------------------
def compute_snr(alphas_cumprod: torch.FloatTensor, timesteps: torch.LongTensor):
    # snr_t = alpha_bar_t / (1 - alpha_bar_t)
    a = alphas_cumprod.to(timesteps.device)[timesteps]  # [B]
    return a / (1.0 - a).clamp(min=1e-8)

def min_snr_weights(snr: torch.FloatTensor, gamma: float):
    # è®ºæ–‡ä¸­å¤šç§å†™æ³•ï¼›è¿™é‡Œé€‰ä¸€ä¸ªå¸¸è§å®ç°
    return torch.minimum(snr, torch.full_like(snr, gamma)) / (snr + 1.0)

# -------------------------
# å­¦ä¹ ç‡è°ƒåº¦ï¼ˆcosine + warmupï¼‰
# -------------------------
class CosineLRScheduler:
    def __init__(self, optimizer, max_steps, warmup_steps=1000, min_lr_ratio=0.1):
        self.opt = optimizer
        self.max_steps = max_steps
        self.warm = warmup_steps
        self.min_ratio = min_lr_ratio
        self.step_idx = 0
        self.base_lrs = [g["lr"] for g in optimizer.param_groups]
    def step(self):
        self.step_idx += 1
        if self.step_idx < self.warm:
            scale = self.step_idx / max(1, self.warm)
        else:
            progress = (self.step_idx - self.warm) / max(1, self.max_steps - self.warm)
            scale = self.min_ratio + 0.5*(1 - self.min_ratio)*(1 + math.cos(math.pi*progress))
        for i, g in enumerate(self.opt.param_groups):
            g["lr"] = self.base_lrs[i] * scale

# -------------------------
# è®­ç»ƒå­å‘½ä»¤
# -------------------------
def cmd_train(args):
    seed_everything(args.seed)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    use_amp = (device.type == "cuda")

    os.makedirs(args.out_dir, exist_ok=True)

    # ====== æ—¥å¿—ä¸ç»˜å›¾é…ç½® ======
    plot_interval   = getattr(args, "plot_interval", 500)
    ema_alpha       = getattr(args, "loss_ema", 0.98)
    csv_path        = os.path.join(args.out_dir, "loss.csv")
    png_path        = os.path.join(args.out_dir, "loss.png")

    # ====== æŸå¤±å†å²ï¼ˆå›ºå®šæœ€å¤š 200000 ç‚¹ï¼‰======
    MAX_POINTS = 200000
    loss_steps: deque[int] = deque(maxlen=MAX_POINTS)
    loss_vals:  deque[float] = deque(maxlen=MAX_POINTS)
    ema_vals:   deque[float] = deque(maxlen=MAX_POINTS)
    ema_state = None

    def read_y_range(file_path="range.txt"):
        """
        å°è¯•ä»æ–‡ä»¶è¯»å– y è½´èŒƒå›´ï¼Œæ ¼å¼è¦æ±‚ï¼š
        min max
        å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–æ ¼å¼é”™è¯¯ï¼Œè¿”å› None
        """
        if not os.path.isfile(file_path):
            return None
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                parts = f.read().strip().split()
                if len(parts) != 2:
                    return None
                ymin, ymax = float(parts[0]), float(parts[1])
                if ymin >= ymax:
                    return None
                return (ymin, ymax)
        except Exception:
            return None

    def log_loss(step: int, val: float):
        nonlocal ema_state
        loss_steps.append(step)
        loss_vals.append(val)
        if ema_alpha >= 1.0:
            ema_vals.append(val)
        else:
            ema_state = val if ema_state is None else (ema_alpha * ema_state + (1 - ema_alpha) * val)
            ema_vals.append(ema_state)

    def flush_csv():
        with open(csv_path, "w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow(["step", "loss", f"loss_ema(alpha={ema_alpha})"])
            for s, v, e in zip(loss_steps, loss_vals, ema_vals):
                w.writerow([s, f"{v:.6f}", f"{e:.6f}"])

    def save_plot():
        if not loss_steps:
            return
        y_range = read_y_range("range.txt")
        plt.figure(figsize=(8,4.5), dpi=150)
        plt.plot(list(loss_steps), list(loss_vals), label="loss", linewidth=1)
        if ema_vals and (ema_alpha < 1.0):
            plt.plot(list(loss_steps), list(ema_vals), label=f"loss EMA (Î±={ema_alpha})", linewidth=1)
        plt.xlabel("step")
        plt.ylabel("loss")
        plt.title("Training Loss")
        plt.legend(loc="best")
        plt.grid(True, linewidth=0.3)
        if y_range is not None:
            plt.ylim(*y_range)
        plt.tight_layout()
        plt.savefig(png_path)
        plt.close()

    # æ•°æ®ï¼ˆä» index.jsonl è¯» npz æ–‡ä»¶å + captionï¼‰
    index_path = os.path.join(args.data_dir, "index.jsonl")
    dataset = LatentCLIPDataset(index_path, args.data_dir, return_caption=True)
    uncond_emb = torch.from_numpy(np.load(os.path.join(args.data_dir, "uncond_emb.npy"))).to(torch.float16)  # (1,77,768)
    print(">> dataset size:", len(dataset))

    loader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.workers,
        pin_memory=(device.type == "cuda"),
        drop_last=True,
        persistent_workers=(args.workers > 0),
        prefetch_factor=2 if args.workers > 0 else None
    )

    # UNet
    print(">> building UNet (SD1.x config)â€¦")
    unet = UNet2DConditionModel.from_config({
        "sample_size": 64, "in_channels": 4, "out_channels": 4,
        "down_block_types": ["CrossAttnDownBlock2D","CrossAttnDownBlock2D","CrossAttnDownBlock2D","DownBlock2D"],
        "up_block_types": ["UpBlock2D","CrossAttnUpBlock2D","CrossAttnUpBlock2D","CrossAttnUpBlock2D"],
        "block_out_channels": [320, 640, 1280, 1280],
        "layers_per_block": 2, "cross_attention_dim": 768,
        "mid_block_type": "UNetMidBlock2DCrossAttn"
    }).to(device)
    unet.enable_gradient_checkpointing()

    # schedulerï¼ˆè®­ç»ƒï¼‰
    prediction_type = "v_prediction" if args.vpred else "epsilon"
    noise_sched = DDPMScheduler(
        num_train_timesteps=1000,
        beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear",
        prediction_type=prediction_type
    )

    # ä¼˜åŒ–å™¨ & EMA & AMP
    base_lr = args.lr
    optimizer = torch.optim.AdamW(unet.parameters(), lr=base_lr, betas=(0.9, 0.999), weight_decay=1e-2)
    ema = EMA(unet, decay=args.ema)
    scaler = torch.amp.GradScaler('cuda', enabled=use_amp)

    # LR schedule â€”â€” æŒ‰â€œä¼˜åŒ–å™¨æ­¥â€è®¡æ•°
    micro_steps_per_epoch = len(loader)
    opt_steps_per_epoch   = math.ceil(micro_steps_per_epoch / max(1, args.grad_accum))
    total_opt_steps       = args.epochs * opt_steps_per_epoch
    warmup_opt_steps = args.warmup if args.warmup > 0 else max(100, int(0.03 * total_opt_steps))
    lr_sched = CosineLRScheduler(
        optimizer,
        max_steps     = total_opt_steps,
        warmup_steps  = warmup_opt_steps,
        min_lr_ratio  = 0.1
    )

    # ===== é¢„è§ˆå›¾ç®¡çº¿ï¼ˆåªæ„å»ºä¸€æ¬¡ï¼‰ =====
    from diffusers import StableDiffusionPipeline
    dtype = torch.float16 if device.type == "cuda" else torch.float32
    pipe = StableDiffusionPipeline.from_pretrained(SD15_REPO, torch_dtype=dtype).to(device)
    pipe.safety_checker = None
    pipe.enable_attention_slicing()
    try:
        pipe.enable_xformers_memory_efficient_attention()
    except Exception:
        pass
    pipe.scheduler.config.prediction_type = prediction_type

    @torch.no_grad()
    def save_preview_image(prompt_text: str, step: int, use_ema: bool = False):
        if not prompt_text:
            return
        print(f"DBG: saving preview images for step {step}, prompt: {prompt_text}")
        # é€‰æ‹©ç”¨ EMA è¿˜æ˜¯åŸå§‹ UNet
        if use_ema:
            tmp_unet = UNet2DConditionModel.from_config(unet.config).to(device, dtype=dtype)
            ema.copy_to(tmp_unet)  # æŠŠ EMA æƒé‡å¤åˆ¶åˆ°ä¸´æ—¶æ¨¡å‹
        else:
            tmp_unet = UNet2DConditionModel.from_config(unet.config).to(device, dtype=dtype)
            tmp_unet.load_state_dict(unet.state_dict(), strict=True)

        pipe.unet = tmp_unet

        g = torch.Generator(device=device.type)
        if args.preview_seed is not None:
            g = g.manual_seed(args.preview_seed)

        image = pipe(
            prompt=prompt_text,
            negative_prompt=getattr(args, "preview_negative", ""),
            num_inference_steps=args.preview_steps,
            guidance_scale=args.preview_scale,
            width=args.preview_size,
            height=args.preview_size,
            generator=g
        ).images[0]

        out_dir = os.path.join(args.out_dir, "preview")
        os.makedirs(out_dir, exist_ok=True)
        prefix = "ema" if use_ema else "raw"
        out_path = os.path.join(out_dir, f"step_{step:08d}_{prefix}.png")
        image.save(out_path)
        print(f">> saved preview: {out_path}")

    # ===== æ¢å¤è®­ç»ƒï¼ˆå¦‚æœä¼ äº† --resumeï¼‰=====
    start_step, start_epoch, pt_from_state, start_opt_step = try_load_train_state(args.resume, unet, optimizer, lr_sched, ema, scaler)
    if pt_from_state is not None and pt_from_state != prediction_type:
        print(f"[warn] resume checkpoint pred_type={pt_from_state} != current {prediction_type}")

    # è®­ç»ƒå¾ªç¯
    global_step = start_step  # å¾®æ­¥
    opt_step = start_opt_step
    unet.train()
    optimizer.zero_grad(set_to_none=True)

    for epoch in range(args.epochs):
        pbar = tqdm(loader, desc=f"epoch {epoch+1}/{args.epochs}")
        for batch in pbar:
            # DataLoader è¿”å› (lat, emb, cap)
            lat, emb, cap = batch
            lat = lat.to(device, dtype=torch.float16)                 # [B,4,64,64], å·²å« 0.18215 ç¼©æ”¾
            emb = emb.to(device, dtype=torch.float16)                 # [B,77,768]

            B = lat.size(0)

            # CFG dropoutï¼šæŒ‰ p_drop æ›¿æ¢ä¸º uncond
            if args.cfg_drop > 0.0:
                mask = (torch.rand(B, device=device) < args.cfg_drop).view(B, 1, 1).to(torch.bool)
                un = uncond_emb.to(device).repeat(B, 1, 1)            # [B,77,768]
                emb = torch.where(mask, un, emb)

            # é‡‡æ · t ä¸å™ªå£°
            t = torch.randint(0, noise_sched.config.num_train_timesteps, (B,), device=device)
            noise = torch.randn_like(lat)
            with torch.amp.autocast('cuda', enabled=use_amp, dtype=torch.float16):
                noisy = noise_sched.add_noise(lat, noise, t)

                # å‰å‘
                pred = unet(noisy, t, encoder_hidden_states=emb).sample

                # ç›®æ ‡
                if prediction_type == "epsilon":
                    target = noise
                else:
                    target = noise_sched.get_velocity(lat, noise, t)

                loss = F.mse_loss(pred, target, reduction="none")
                loss = loss.mean(dim=(1,2,3))  # [B]

                # Min-SNR reweightingï¼ˆå¯é€‰ï¼‰
                if args.min_snr_gamma > 0:
                    snr = compute_snr(noise_sched.alphas_cumprod, t)  # [B]
                    w = min_snr_weights(snr, gamma=args.min_snr_gamma)
                    loss = (loss * w).mean()
                else:
                    loss = loss.mean()

            # ====== è®°å½• loss & ç”»å›¾ ======
            loss_scalar = float(loss.detach().cpu())
            log_loss(global_step + 1, loss_scalar)
            if (global_step + 1) % plot_interval == 0:
                flush_csv()
                save_plot()

            scaler.scale(loss / args.grad_accum).backward()

            # æ¢¯åº¦ç´¯ç§¯ -> ä¼˜åŒ–å™¨æ­¥
            if (global_step + 1) % args.grad_accum == 0:
                nn.utils.clip_grad_norm_(unet.parameters(), 1.0)
                scaler.step(optimizer); scaler.update()
                optimizer.zero_grad(set_to_none=True)
                lr_sched.step()
                ema.update(unet)
                opt_step += 1

            global_step += 1
            pbar.set_postfix({"loss": float(loss.detach().cpu()), "lr": optimizer.param_groups[0]["lr"]})

            # ä¿å­˜ï¼ˆæŒ‰å¾®æ­¥ step å‘½åï¼‰
            if args.save_steps and (global_step % args.save_steps == 0):
                save_ckpt(args, unet, ema, step=global_step, prediction_type=prediction_type)
                # åŒæ­¥ä¿å­˜é¢„è§ˆå›¾ï¼ˆä½¿ç”¨å½“å‰ batch çš„ç¬¬ä¸€ä¸ª captionï¼‰
                if getattr(args, "preview_every_ckpt", False):
                    if isinstance(cap, (list, tuple)):
                        prompt_text = cap[0]
                    else:
                        prompt_text = cap
                    # åŸå§‹ UNet
                    save_preview_image(prompt_text, global_step, use_ema=False)
                    # EMA æƒé‡
                    save_preview_image(prompt_text, global_step, use_ema=True)

        # æ¯ä¸ª epoch ç»“æŸä¹Ÿä¿å­˜ä¸€æ¬¡
        if args.save_epochs and (epoch + 1) % args.save_epochs == 0:
            save_ckpt(args, unet, ema, step=global_step, prediction_type=prediction_type)
            if getattr(args, "preview_every_ckpt", False):
                # å°è¯•é‡å¤ä½¿ç”¨æœ€è¿‘ä¸€æ¬¡çš„ promptï¼ˆè‹¥æœ‰ï¼‰
                if 'prompt_text' in locals() and prompt_text:
                    save_preview_image(prompt_text, global_step, use_ema=False)
                    save_preview_image(prompt_text, global_step, use_ema=True)

    print(">> training done.")

def save_ckpt(args, unet, ema: EMA, step: int, prediction_type: str):
    # ä¿å­˜å½“å‰ raw æƒé‡
    raw_dir = os.path.join(args.out_dir, f"step_{step}_raw")
    os.makedirs(raw_dir, exist_ok=True)
    torch.save(unet.state_dict(), os.path.join(raw_dir, "unet_raw.pt"))

    # ä¿å­˜ EMA æƒé‡ï¼ˆæ¨èæ¨ç†ç”¨ï¼‰
    ema_model = UNet2DConditionModel.from_config(unet.config).to(unet.device)
    ema.copy_to(ema_model)
    ema_dir = os.path.join(args.out_dir, f"step_{step}_ema")
    os.makedirs(ema_dir, exist_ok=True)

    # ç”¨ safetensors å­˜ä¸€ä»½
    state = {k: v.detach().cpu() for k, v in ema_model.state_dict().items()}
    safetensors_save(state, os.path.join(ema_dir, "unet_ema.safetensors"))

    # ä¿å­˜ config å’Œ meta.json
    with open(os.path.join(ema_dir, "config.json"), "w", encoding="utf-8") as f:
        json.dump(unet.config, f, indent=2)
    with open(os.path.join(ema_dir, "meta.json"), "w", encoding="utf-8") as f:
        json.dump({"prediction_type": prediction_type, "step": step}, f, indent=2)

    print(f">> saved ckpt @ {ema_dir}")

    # â›ï¸ è‡ªåŠ¨æ¸…ç†æ—§çš„ checkpointï¼ˆåªä¿ç•™æœ€è¿‘10ä¸ªï¼‰
    ckpt_dirs = sorted(
        glob.glob(os.path.join(args.out_dir, "step_*_ema")),
        key=os.path.getmtime
    )
    max_keep = 3
    if len(ckpt_dirs) > max_keep:
        for old_dir in ckpt_dirs[:-max_keep]:
            try:
                # åˆ é™¤æ•´ä¸ª EMA ckpt æ–‡ä»¶å¤¹
                for file in glob.glob(os.path.join(old_dir, "*")):
                    os.remove(file)
                os.rmdir(old_dir)
                print(f"ğŸ—‘ï¸ deleted old ema ckpt: {old_dir}")
            except Exception as e:
                print(f"âŒ failed to delete {old_dir}: {e}")

        # å¯é€‰ï¼šä¹Ÿæ¸…ç†å¯¹åº”çš„ raw_ckpt
        raw_dirs = sorted(
            glob.glob(os.path.join(args.out_dir, "step_*_raw")),
            key=os.path.getmtime
        )
        for old_dir in raw_dirs[:-max_keep]:
            try:
                for file in glob.glob(os.path.join(old_dir, "*")):
                    os.remove(file)
                os.rmdir(old_dir)
                print(f"ğŸ—‘ï¸ deleted old raw ckpt: {old_dir}")
            except Exception as e:
                print(f"âŒ failed to delete {old_dir}: {e}")

# -------------------------
# æ¨ç†å­å‘½ä»¤ï¼ˆdiffusers ç®¡çº¿ï¼Œæ›¿æ¢ UNetï¼‰
# -------------------------
def cmd_infer(args):
    from diffusers import StableDiffusionPipeline, DDIMScheduler
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # è½½å…¥ç®¡çº¿
    dtype = torch.float16 if device.type == "cuda" else torch.float32
    pipe = StableDiffusionPipeline.from_pretrained(SD15_REPO, torch_dtype=dtype).to(device)
    pipe.safety_checker = None  # å¯é€‰
    pipe.enable_attention_slicing()
    try:
        pipe.enable_xformers_memory_efficient_attention()
    except Exception:
        pass

    # åŠ è½½è‡ªè®­ UNet
    print(">> loading trained UNetâ€¦")
    unet = UNet2DConditionModel.from_config(pipe.unet.config).to(device, dtype=dtype)

    # æ—¢æ”¯æŒ .safetensors ä¹Ÿæ”¯æŒ .pt
    if args.unet_path.endswith(".safetensors"):
        sd = safetensors_load(args.unet_path)
    else:
        sd = torch.load(args.unet_path, map_location="cpu")
    missing, unexpected = unet.load_state_dict(sd, strict=False)
    print("missing:", len(missing), "unexpected:", len(unexpected))
    pipe.unet = unet

    # scheduler é…ç½®è¦å’Œè®­ç»ƒå¯¹é½
    if args.vpred:
        pipe.scheduler.config.prediction_type = "v_prediction"
    else:
        pipe.scheduler.config.prediction_type = "epsilon"

    # å‡ºå›¾
    g = torch.Generator(device=device.type)
    if args.seed is not None:
        g = g.manual_seed(args.seed)

    image = pipe(
        prompt=args.prompt,
        negative_prompt=args.negative_prompt,
        num_inference_steps=args.steps,
        guidance_scale=args.scale,
        width=args.width, height=args.height,
        generator=g
    ).images[0]

    os.makedirs(os.path.dirname(args.out), exist_ok=True)
    image.save(args.out)
    print(">> saved", args.out)

# -------------------------
# CLI
# -------------------------
def build_parser():
    p = argparse.ArgumentParser("SD1.5 UNet Train & Infer (single file)")
    sub = p.add_subparsers(dest="cmd")

    # encode
    pe = sub.add_parser("encode", help="é¢„ç¼–ç å›¾åƒä¸º latent & æ–‡æœ¬åµŒå…¥")
    pe.add_argument("--data_dir", type=str, default=None, help="å›¾åƒç›®å½•ï¼ˆè‹¥ä¸æä¾› --csvï¼‰")
    pe.add_argument("--csv", type=str, default=None, help="CSV with columns: path,caption")
    pe.add_argument("--out_dir", type=str, required=True)
    pe.add_argument("--size", type=int, default=512)
    pe.set_defaults(func=cmd_encode)

    # train
    pt = sub.add_parser("train", help="è®­ç»ƒ UNetï¼ˆä»… latent ç©ºé—´ï¼‰")
    pt.add_argument("--data_dir", type=str, required=True, help="encode äº§ç”Ÿçš„æ•°æ®ç›®å½•ï¼ˆå« index.jsonl/uncond_emb.npyï¼‰")
    pt.add_argument("--out_dir", type=str, required=True)
    pt.add_argument("--batch_size", type=int, default=8)
    pt.add_argument("--workers", type=int, default=4)
    pt.add_argument("--epochs", type=int, default=5)
    pt.add_argument("--lr", type=float, default=2e-4)
    pt.add_argument("--warmup", type=int, default=1000)
    pt.add_argument("--grad_accum", type=int, default=1)
    pt.add_argument("--ema", type=float, default=0.999)
    pt.add_argument("--cfg_drop", type=float, default=0.1, help="Classifier-Free Guidance è®­ç»ƒæ—¶çš„ unconditional dropout æ¦‚ç‡")
    pt.add_argument("--min_snr_gamma", type=float, default=0.0, help=">0 å¯ç”¨ Min-SNR æƒé‡ï¼ˆå¦‚ 5.0ï¼‰")
    pt.add_argument("--vpred", action="store_true", help="ä½¿ç”¨ v-prediction è®­ç»ƒï¼ˆé»˜è®¤ epsilonï¼‰")
    pt.add_argument("--seed", type=int, default=1234)
    pt.add_argument("--save_steps", type=int, default=2000)
    pt.add_argument("--save_epochs", type=int, default=0, help=">0 åˆ™æ¯ N ä¸ª epoch ä¿å­˜ä¸€æ¬¡")

    pt.add_argument("--plot_interval", type=int, default=500, help="æ¯éš”å¤šå°‘ step ä¿å­˜ä¸€æ¬¡æŸå¤±æ›²çº¿")
    pt.add_argument("--loss_ema", type=float, default=0.98, help="loss EMA å¹³æ»‘ç³»æ•°ï¼Œ1.0 è¡¨ç¤ºå…³é—­å¹³æ»‘")

    pt.add_argument("--resume", type=str, default=None, help="ä»ä¿å­˜çš„è®­ç»ƒçŠ¶æ€æ¢å¤ï¼ˆ.pth/.pt æˆ–ç›®å½•ï¼‰")

    # é¢„è§ˆå›¾ç›¸å…³å‚æ•°
    pt.add_argument("--preview_every_ckpt", action="store_true", help="ä¿å­˜æƒé‡æ—¶åŒæ—¶ä¿å­˜é¢„è§ˆå›¾")
    pt.add_argument("--preview_steps", type=int, default=30, help="é¢„è§ˆå›¾æ¨ç†æ­¥æ•°")
    pt.add_argument("--preview_scale", type=float, default=7.5, help="CFG scale")
    pt.add_argument("--preview_seed", type=int, default=12345, help="é¢„è§ˆå›¾éšæœºç§å­")
    pt.add_argument("--preview_size", type=int, default=512, help="é¢„è§ˆå›¾è¾¹é•¿(æ­£æ–¹å½¢)")
    pt.add_argument("--preview_negative", type=str, default="", help="é¢„è§ˆå›¾çš„è´Ÿé¢æç¤ºè¯")

    pt.set_defaults(func=cmd_train)

    # infer
    pi = sub.add_parser("infer", help="æ›¿æ¢ç®¡çº¿ UNet æ¨ç†å‡ºå›¾")
    pi.add_argument("--unet_path", type=str, required=True, help="è®­ç»ƒäº§å‡ºçš„ EMA/RAW æƒé‡ï¼ˆ.safetensors æˆ– .ptï¼‰")
    pi.add_argument("--prompt", type=str, required=True)
    pi.add_argument("--negative_prompt", type=str, default="")
    pi.add_argument("--steps", type=int, default=30)
    pi.add_argument("--scale", type=float, default=7.5)
    pi.add_argument("--width", type=int, default=512)
    pi.add_argument("--height", type=int, default=512)
    pi.add_argument("--seed", type=int, default=None)
    pi.add_argument("--vpred", action="store_true", help="æ¨ç†ç”¨ v-predictionï¼ˆéœ€ä¸è®­ç»ƒå¯¹é½ï¼‰")
    pi.add_argument("--out", type=str, default="out.png")
    pi.set_defaults(func=cmd_infer)

    # decode
    pd = sub.add_parser("decode", help="å°† latent(.npz) è§£ç ä¸ºå›¾ç‰‡ï¼ˆä½¿ç”¨ SD1.5 çš„ VAEï¼‰")
    pd.add_argument("--input", type=str, required=True, help="å•ä¸ª .npz æ–‡ä»¶æˆ–ç›®å½•")
    pd.add_argument("--out", type=str, default=None, help="å•æ–‡ä»¶æ¨¡å¼çš„è¾“å‡ºè·¯å¾„ï¼ˆ.pngï¼‰")
    pd.add_argument("--out_dir", type=str, default="./decoded", help="ç›®å½•æ¨¡å¼ä¸‹çš„è¾“å‡ºç›®å½•")
    pd.add_argument("--scale", type=float, default=VAE_SCALE, help="latent ç¼©æ”¾ï¼ˆSD1.x=0.18215ï¼‰")
    pd.set_defaults(func=cmd_decode)

    return p

def main():
    parser = build_parser()
    args = parser.parse_args()
    if not hasattr(args, "func"):
        print("Use one of subcommands: encode | train | infer. Example:\n"
              "  python sd15_unet_train_infer.py encode --csv data.csv --out_dir ./latent_db\n"
              "  python sd15_unet_train_infer.py train --data_dir ./latent_db --out_dir ./ckpts --vpred --min_snr_gamma 5 --preview_every_ckpt\n"
              "  python sd15_unet_train_infer.py infer --unet_path ./ckpts/step_20000_ema/unet_ema.safetensors "
              "--prompt 'best quality, 1girl, miko' --vpred --out sample.png")
        return
    args.func(args)

if __name__ == "__main__":
    main()
