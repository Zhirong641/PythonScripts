# -*- coding: utf-8 -*-
import os, io, math, json, glob, argparse, random, hashlib
from typing import List
import numpy as np
from PIL import Image

import torch
import torch.nn.functional as F
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from tqdm import tqdm

from diffusers import UNet2DConditionModel, DDPMScheduler, AutoencoderKL
from transformers import CLIPTokenizer, CLIPTextModel
from safetensors.torch import save_file as safetensors_save, load_file as safetensors_load

from CSVProcessor import CSVProcessor  # éœ€æä¾›ï¼šè¿”å›æ¯è¡Œ [path, caption, author]

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import csv as pycsv
from collections import deque, OrderedDict

# -------------------------
# å…¨å±€å¸¸é‡ï¼ˆSD1.xæƒ¯ä¾‹ï¼‰
# -------------------------
VAE_SCALE = 0.18215
MAX_TOKEN = 77
CLIP_MODEL = "openai/clip-vit-large-patch14"
SD15_REPO = "runwayml/stable-diffusion-v1-5"

# 5 æ¡¶ï¼ˆå»æ‰ç©ºæç¤ºï¼‰
VARIANT_NAMES = ["tags_fwd", "tags_rev", "author", "both_fwd", "both_rev"]
PROBS_5 = np.array([0.3, 0.15, 0.1, 0.3, 0.15], dtype=np.float64)

# -------------------------
# å›¾åƒé¢„å¤„ç†ï¼ˆPaddingåˆ° 512x512ï¼‰
# -------------------------
def make_image_transform(resolution=512):
    def resize_pad(img: Image.Image) -> Image.Image:
        w, h = img.size
        scale = resolution / max(w, h)
        new_w, new_h = int(w * scale), int(h * scale)
        img = img.resize((new_w, new_h), Image.BICUBIC)
        from PIL import ImageOps
        pad_w = (resolution - new_w) // 2
        pad_h = (resolution - new_h) // 2
        pad = (pad_w, pad_h, resolution - new_w - pad_w, resolution - new_h - pad_h)
        img = ImageOps.expand(img, border=pad, fill=(0, 0, 0))
        return img

    return transforms.Compose([
        transforms.Lambda(resize_pad),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5])  # [0,1] â†’ [-1,1]
    ])

def save_tensor_image(img_t: torch.Tensor, path: str):
    img = (img_t.clamp(-1, 1) * 0.5 + 0.5)
    img = (img * 255.0).round().byte().cpu().permute(0,2,3,1).numpy()
    os.makedirs(os.path.dirname(path), exist_ok=True)
    from PIL import Image
    Image.fromarray(img[0]).save(path)

# -------------------------
# å®ç”¨å‡½æ•°
# -------------------------
def seed_everything(seed: int):
    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = True
    try:
        torch.set_float32_matmul_precision('high')
    except Exception:
        pass

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8")).hexdigest()

def _split_clean_comma_list(s: str) -> List[str]:
    if not s: return []
    items = [x.strip() for x in s.replace("ï¼Œ", ",").split(",")]
    return [x for x in items if x]

def _join_with_comma(items: List[str]) -> str:
    return ", ".join(items)

def _build_variants_from_cap_author(caption: str, author: str):
    """
    åŸºäºåŸå§‹ caption/author æ„é€  5 ç±»å˜ä½“æ–‡æœ¬ï¼Œå¹¶ç”Ÿæˆå¯ç”¨ maskï¼ˆå»é‡åï¼‰ã€‚
    è¿”å› texts(list[str] é•¿åº¦5), mask(np.bool_ é•¿åº¦5), preview_text(str)
    """
    tags = _split_clean_comma_list(caption)
    auth = _split_clean_comma_list(author)

    tags_fwd = _join_with_comma(tags) if tags else ""
    tags_rev = _join_with_comma(list(reversed(tags))) if tags else ""
    author_s = _join_with_comma(auth) if auth else ""

    if author_s and tags_fwd:
        both_fwd = f"{author_s}, {tags_fwd}"
    else:
        both_fwd = author_s or tags_fwd

    if author_s and tags_rev:
        both_rev = f"{author_s}, {tags_rev}"
    else:
        both_rev = author_s or tags_rev

    texts = [tags_fwd, tags_rev, author_s, both_fwd, both_rev]
    mask = [bool(t) for t in texts]

    # å»é‡ï¼šåå‡ºç°è€…è‹¥ä¸å‰è€…ç›¸åŒï¼Œåˆ™å±è”½
    for j in range(len(texts)):
        for k in range(j):
            if mask[j] and mask[k] and texts[j] == texts[k]:
                mask[j] = False

    mask = np.array(mask, dtype=np.bool_)
    preview_text = both_fwd if mask[3] else (tags_fwd if mask[0] else (author_s if mask[2] else ""))

    return texts, mask, preview_text

# -------------------------
# é¢„ç¼–ç ï¼šåªä¿å­˜ latentï¼Œindex.jsonl ä»…å†™å…¥ npz/caption/author
# -------------------------
def cmd_encode(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    os.makedirs(args.out_dir, exist_ok=True)
    tfm = make_image_transform(args.size)

    print(">> loading VAEâ€¦")
    vae: AutoencoderKL = AutoencoderKL.from_pretrained(
        SD15_REPO, subfolder="vae", torch_dtype=torch.float16
    ).to(device)
    vae.requires_grad_(False).eval()

    if not args.csv:
        raise ValueError("è¯·ä½¿ç”¨ --csv data.csvï¼Œå¹¶ä¿è¯åˆ—ä¸º path,caption,author")
    processor = CSVProcessor(args.csv)
    data = processor.get_data()
    print("CSV rows:", len(data))
    if data:
        print("First row:", data[0])

    # å¯é€‰è¿‡æ»¤
    exclude_word_list = ["no humans", "chibi", "character profile", "lineart",
                        "sketch", "monochrome", "comic", "text focus", "1990s", "1980s",
                        "retro artstyle", "abstract"]

    # # debug
    # rows = processor.get_rows_by_value("path", "webp/2588678", False)
    # rows = rows[0:1]
    # data = rows

    samples = []
    for row in data:
        try:
            path = row[0]
            caption = row[1] if len(row) > 1 else ""
            author = row[2] if len(row) > 2 else ""
            if any(ex in caption for ex in exclude_word_list):
                continue
            samples.append((path, caption, author))
        except Exception:
            continue
    random.shuffle(samples)

    index_path = os.path.join(args.out_dir, "index.jsonl")
    idxf = open(index_path, "w", encoding="utf-8")

    print(f">> encoding {len(samples)} samplesâ€¦")
    for (path, caption, author) in tqdm(samples):
        try:
            img = Image.open(path).convert("RGB")
        except Exception as e:
            print(f"[skip] open {path}: {e}")
            continue

        pixel = tfm(img).unsqueeze(0).to(device)  # [1,3,512,512]
        with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.float16, enabled=(device.type=="cuda")):
            lat = vae.encode(pixel).latent_dist.sample() * VAE_SCALE  # [1,4,64,64]

        lat = lat[0].detach().cpu().to(torch.float16).numpy()
        base = sha1(path) + ".npz"
        np.savez_compressed(os.path.join(args.out_dir, base), latent=lat, src=np.bytes_(path))

        # ä»…å†™åŸå§‹ caption/authorï¼ˆä¸å†™ä»»ä½•å˜ä½“ï¼‰
        meta = {"npz": base, "src": path, "caption": caption, "author": author}
        idxf.write(json.dumps(meta, ensure_ascii=False) + "\n")

    idxf.close()
    print(">> done. Saved to", args.out_dir)

# -------------------------
# è§£ç  latent -> å›¾ç‰‡ï¼ˆè°ƒè¯•ç”¨ï¼‰
# -------------------------
def cmd_decode(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    dtype = torch.float16 if device.type == "cuda" else torch.float32

    vae: AutoencoderKL = AutoencoderKL.from_pretrained(
        SD15_REPO, subfolder="vae", torch_dtype=dtype
    ).to(device)
    vae.eval().requires_grad_(False)

    def decode_one(npz_path: str, out_path: str):
        z = np.load(npz_path)
        assert "latent" in z, f"{npz_path} ä¸å« 'latent'"
        lat = torch.from_numpy(z["latent"]).to(device=device, dtype=dtype).unsqueeze(0)  # [1,4,H,W]
        lat = lat / args.scale
        with torch.no_grad(), torch.amp.autocast('cuda', enabled=(dtype==torch.float16), dtype=dtype):
            img = vae.decode(lat).sample  # [-1,1]
        save_tensor_image(img, out_path)
        print(f"decoded -> {out_path}, latent shape: {lat.shape}, image shape: {img.shape}")

    if os.path.isdir(args.input):
        files = sorted([p for p in glob.glob(os.path.join(args.input, "*.npz"))])
        assert files, f"{args.input} ä¸‹æ²¡æœ‰ .npz"
        os.makedirs(args.out_dir, exist_ok=True)
        for p in files:
            base = os.path.splitext(os.path.basename(p))[0]
            out = os.path.join(args.out_dir, base + ".png")
            decode_one(p, out)
    else:
        in_path = args.input
        out_path = args.out or (os.path.splitext(in_path)[0] + ".png")
        decode_one(in_path, out_path)

# -------------------------
# æ•°æ®é›†ï¼šè¯»å– latent + åŸå§‹ caption/author
# -------------------------
class LatentCapAuthorDataset(Dataset):
    """
    ä» index.jsonl è¯»å– (npz_filename, caption, author)ã€‚
    __getitem__ è¿”å›ï¼š
      lat: [4,64,64] float16
      caption: str
      author: str
      preview_text: strï¼ˆåŸºäº caption/author ç°ç®—ï¼Œç”¨äºä¿å­˜é¢„è§ˆï¼‰
    """
    def __init__(self, index_jsonl: str, root_dir: str):
        self.root = root_dir
        self.items = []
        if not os.path.isfile(index_jsonl):
            raise FileNotFoundError(f"index file not found: {index_jsonl}")
        with open(index_jsonl, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    j = json.loads(line)
                    fname = j["npz"]
                    cap = j.get("caption", "")
                    auth = j.get("author", "")
                    full_path = os.path.join(root_dir, fname)
                    if not os.path.isfile(full_path):
                        continue
                    # ä¸ºäº† DataLoader é¢„è§ˆï¼Œé¢„å…ˆè®¡ç®—ä¸€æ¬¡ preview_text
                    _, _, preview = _build_variants_from_cap_author(cap, auth)
                    self.items.append((fname, cap, auth, preview))
                except Exception:
                    continue
        print(f"Loaded {len(self.items)} items from {index_jsonl}")

    def __len__(self): return len(self.items)

    def __getitem__(self, i):
        fname, cap, auth, preview = self.items[i]
        z = np.load(os.path.join(self.root, fname), allow_pickle=False)
        lat = z["latent"].astype(np.float16)          # (4,64,64)
        return torch.from_numpy(lat), cap, auth, preview

# -------------------------
# EMA
# -------------------------
class EMA:
    def __init__(self, model: nn.Module, decay: float = 0.999):
        self.decay = decay
        self.shadow = {}
        for n, p in model.named_parameters():
            if p.requires_grad:
                self.shadow[n] = p.data.detach().clone()
    @torch.no_grad()
    def update(self, model: nn.Module):
        for n, p in model.named_parameters():
            if not p.requires_grad:
                continue
            self.shadow[n].mul_(self.decay).add_(p.data, alpha=1.0 - self.decay)
    def copy_to(self, model: nn.Module):
        for n, p in model.named_parameters():
            if n in self.shadow:
                p.data.copy_(self.shadow[n])
    def state_dict(self):
        return {"decay": self.decay, "shadow": {k: v.cpu() for k, v in self.shadow.items()}}
    def load_state_dict(self, state):
        self.decay = state["decay"]
        sh = state["shadow"]
        for n, p in self.shadow.items():
            if n in sh:
                self.shadow[n] = sh[n].to(p.device, dtype=p.dtype)

def save_train_state(path, unet, optimizer, lr_sched, ema: EMA, global_step, epoch, prediction_type, scaler=None, opt_step=0):
    pkg = {
        "model": {k: v.detach().cpu() for k, v in unet.state_dict().items()},
        "optimizer": optimizer.state_dict(),
        "lr_sched_step": lr_sched.step_idx if hasattr(lr_sched, "step_idx") else 0,
        "ema": ema.state_dict(),
        "global_step": global_step,
        "epoch": epoch,
        "prediction_type": prediction_type,
        "opt_step": opt_step,
        "scaler": (scaler.state_dict() if (scaler is not None and hasattr(scaler, "state_dict")) else None),
    }
    os.makedirs(os.path.dirname(path), exist_ok=True)
    torch.save(pkg, path)
    print(f">> saved train state: {path}")

def try_load_train_state(resume_path, unet, optimizer, lr_sched, ema: EMA, scaler=None):
    def _load_pkg(p):
        print(f">> resume from: {p}")
        pkg = torch.load(p, map_location="cpu")
        unet.load_state_dict(pkg["model"], strict=True)
        if "optimizer" in pkg and len(pkg["optimizer"]) > 0:
            optimizer.load_state_dict(pkg["optimizer"])
        if "lr_sched_step" in pkg:
            lr_sched.step_idx = pkg["lr_sched_step"]
        if "ema" in pkg:
            ema.load_state_dict(pkg["ema"])
        if scaler is not None and pkg.get("scaler") is not None:
            try:
                scaler.load_state_dict(pkg["scaler"])
            except Exception as e:
                print(f"[warn] scaler state not restored: {e}")
        return int(pkg.get("global_step", 0)), int(pkg.get("epoch", 0)), pkg.get("prediction_type", "epsilon"), int(pkg.get("opt_step", 0))

    if resume_path is None:
        return 0, 0, None, 0

    if os.path.isdir(resume_path):
        cands = [os.path.join(resume_path, x) for x in os.listdir(resume_path) if x.endswith(".pth") or x.endswith(".pt")]
        if not cands:
            raise FileNotFoundError(f"No *.pth found in {resume_path}")
        cands.sort()
        return _load_pkg(cands[-1])

    if resume_path.endswith(".pth") or resume_path.endswith(".pt"):
        try:
            return _load_pkg(resume_path)
        except Exception:
            print(">> resume file is raw model only; loading UNet weights")
            sd = torch.load(resume_path, map_location="cpu")
            unet.load_state_dict(sd, strict=False)
            return 0, 0, None, 0

    raise FileNotFoundError(resume_path)

# -------------------------
# Min-SNR æƒé‡
# -------------------------
def compute_snr(alphas_cumprod: torch.FloatTensor, timesteps: torch.LongTensor):
    a = alphas_cumprod.to(timesteps.device)[timesteps]  # [B]
    return a / (1.0 - a).clamp(min=1e-8)

def min_snr_weights(snr: torch.FloatTensor, gamma: float):
    return torch.minimum(snr, torch.full_like(snr, gamma)) / (snr + 1.0)

# -------------------------
# å­¦ä¹ ç‡è°ƒåº¦ï¼ˆcosine + warmupï¼‰
# -------------------------
class CosineLRScheduler:
    def __init__(self, optimizer, max_steps, warmup_steps=1000, min_lr_ratio=0.1):
        self.opt = optimizer
        self.max_steps = max_steps
        self.warm = warmup_steps
        self.min_ratio = min_lr_ratio
        self.step_idx = 0
        self.base_lrs = [g["lr"] for g in optimizer.param_groups]
    def step(self):
        self.step_idx += 1
        if self.step_idx < self.warm:
            scale = self.step_idx / max(1, self.warm)
        else:
            progress = (self.step_idx - self.warm) / max(1, self.max_steps - self.warm)
            scale = self.min_ratio + 0.5*(1 - self.min_ratio)*(1 + math.cos(math.pi*progress))
        for i, g in enumerate(self.opt.param_groups):
            g["lr"] = self.base_lrs[i] * scale

# -------------------------
# è®­ç»ƒï¼ˆ5 æ¡¶ + è®­ç»ƒæœŸç°ç®—æ–‡æœ¬åµŒå…¥ + CFG dropout æ›¿ä»£ 10% ç©ºæç¤ºï¼‰
# -------------------------
def cmd_train(args):
    seed_everything(args.seed)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    use_amp = (device.type == "cuda")

    os.makedirs(args.out_dir, exist_ok=True)

    # ====== æ—¥å¿—ä¸ç»˜å›¾é…ç½® ======
    plot_interval   = getattr(args, "plot_interval", 500)
    ema_alpha       = getattr(args, "loss_ema", 0.98)
    csv_path        = os.path.join(args.out_dir, "loss.csv")
    png_path        = os.path.join(args.out_dir, "loss.png")

    # ====== æŸå¤±å†å² ======
    MAX_POINTS = 500000
    loss_steps: deque[int] = deque(maxlen=MAX_POINTS)
    loss_vals:  deque[float] = deque(maxlen=MAX_POINTS)
    ema_vals:   deque[float] = deque(maxlen=MAX_POINTS)
    ema_state = None

    def read_y_range(file_path="range.txt"):
        if not os.path.isfile(file_path):
            return None
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                parts = f.read().strip().split()
                if len(parts) != 2:
                    return None
                ymin, ymax = float(parts[0]), float(parts[1])
                if ymin >= ymax:
                    return None
                return (ymin, ymax)
        except Exception:
            return None

    def log_loss(step: int, val: float):
        nonlocal ema_state
        loss_steps.append(step)
        loss_vals.append(val)
        if ema_alpha >= 1.0:
            ema_vals.append(val)
        else:
            ema_state = val if ema_state is None else (ema_alpha * ema_state + (1 - ema_alpha) * val)
            ema_vals.append(ema_state)

    def flush_csv():
        with open(csv_path, "w", newline="", encoding="utf-8") as f:
            w = pycsv.writer(f)
            w.writerow(["step", "loss", f"loss_ema(alpha={ema_alpha})"])
            for s, v, e in zip(loss_steps, loss_vals, ema_vals):
                w.writerow([s, f"{v:.6f}", f"{e:.6f}"])

    def save_plot():
        if not loss_steps:
            return
        y_range = read_y_range("range.txt")
        plt.figure(figsize=(8,4.5), dpi=150)
        plt.plot(list(loss_steps), list(loss_vals), label="loss", linewidth=1)
        if ema_vals and (ema_alpha < 1.0):
            plt.plot(list(loss_steps), list(ema_vals), label=f"loss EMA (Î±={ema_alpha})", linewidth=1)
        plt.xlabel("step"); plt.ylabel("loss"); plt.title("Training Loss")
        plt.legend(loc="best"); plt.grid(True, linewidth=0.3)
        if y_range is not None:
            plt.ylim(*y_range)
        plt.tight_layout(); plt.savefig(png_path); plt.close()

    # ====== æ•°æ® ======
    index_path = os.path.join(args.data_dir, "index.jsonl")
    dataset = LatentCapAuthorDataset(index_path, args.data_dir)
    print(">> dataset size:", len(dataset))

    loader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.workers,
        pin_memory=(device.type == "cuda"),
        drop_last=True,
        persistent_workers=(args.workers > 0),
        prefetch_factor=2 if args.workers > 0 else None
    )

    # ====== UNet ======
    print(">> building UNet (SD1.x config)â€¦")
    unet = UNet2DConditionModel.from_config({
        "sample_size": 64, "in_channels": 4, "out_channels": 4,
        "down_block_types": ["CrossAttnDownBlock2D","CrossAttnDownBlock2D","CrossAttnDownBlock2D","DownBlock2D"],
        "up_block_types": ["UpBlock2D","CrossAttnUpBlock2D","CrossAttnUpBlock2D","CrossAttnUpBlock2D"],
        "block_out_channels": [320, 640, 1280, 1280],
        "layers_per_block": 2, "cross_attention_dim": 768,
        "mid_block_type": "UNetMidBlock2DCrossAttn"
    }).to(device)
    unet.enable_gradient_checkpointing()

    # ====== è°ƒåº¦å™¨ ======
    prediction_type = "v_prediction" if args.vpred else "epsilon"
    noise_sched = DDPMScheduler(
        num_train_timesteps=1000,
        beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear",
        prediction_type=prediction_type
    )

    # ====== ä¼˜åŒ–å™¨ & EMA & AMP ======
    base_lr = args.lr
    optimizer = torch.optim.AdamW(unet.parameters(), lr=base_lr, betas=(0.9, 0.999), weight_decay=1e-2)
    ema = EMA(unet, decay=args.ema)
    scaler = torch.amp.GradScaler('cuda', enabled=use_amp)

    # ====== LR scheduleï¼ˆæŒ‰ä¼˜åŒ–å™¨æ­¥ï¼‰ ======
    micro_steps_per_epoch = len(loader)
    opt_steps_per_epoch   = max(1, math.ceil(micro_steps_per_epoch / max(1, args.grad_accum)))
    total_opt_steps       = max(1, args.epochs * opt_steps_per_epoch)
    warmup_opt_steps = args.warmup if args.warmup > 0 else max(100, int(0.03 * total_opt_steps))
    lr_sched = CosineLRScheduler(optimizer, max_steps=total_opt_steps, warmup_steps=warmup_opt_steps, min_lr_ratio=0.1)

    # ====== é¢„è§ˆå›¾ç®¡çº¿ ======
    from diffusers import StableDiffusionPipeline
    dtype = torch.float16 if device.type == "cuda" else torch.float32
    pipe = StableDiffusionPipeline.from_pretrained(SD15_REPO, torch_dtype=dtype).to(device)
    pipe.safety_checker = None
    pipe.enable_attention_slicing()
    try:
        pipe.enable_xformers_memory_efficient_attention()
    except Exception:
        pass
    pipe.scheduler.config.prediction_type = prediction_type

    @torch.no_grad()
    def save_preview_image(prompt_text: str, step: int, use_ema: bool = False):
        """
        é¢„è§ˆå›¾ä½¿ç”¨è®­ç»ƒæœŸåŒä¸€å¥— tokenizer/text_encoderï¼š
        - cond = encode_texts([prompt_text])
        - neg  = encode_texts([args.preview_negative]) æˆ– uncond_embï¼ˆç©ºè´Ÿé¢ï¼‰
        é€šè¿‡ prompt_embeds / negative_prompt_embeds èµ°ç®¡çº¿ï¼Œé¿å…ä¸ pipe è‡ªå¸¦ CLIP ä¸ä¸€è‡´ã€‚
        """
        if not prompt_text:
            return
        print(f"DBG: saving preview images for step {step}, prompt: {prompt_text}")

        # 1) ç”¨ RAW / EMA æƒé‡æ„å»ºä¸´æ—¶ UNet å¹¶æŒ‚åˆ° pipe ä¸Š
        tmp_unet = UNet2DConditionModel.from_config(unet.config).to(device, dtype=dtype)
        if use_ema:
            ema.copy_to(tmp_unet)
        else:
            tmp_unet.load_state_dict(unet.state_dict(), strict=True)
        pipe.unet = tmp_unet

        # 2) ç”¨è®­ç»ƒæœŸåŒä¸€å¥— CLIP ç”Ÿæˆæ–‡æœ¬åµŒå…¥
        cond_emb = encode_texts([prompt_text])                          # [1,77,768]
        neg_text = getattr(args, "preview_negative", "")
        neg_emb  = encode_texts([neg_text]) if neg_text else uncond_emb # [1,77,768]

        # ç»´åº¦é˜²å‘†ï¼ˆé¿å…æ¢äº† CLIP ç»´åº¦ä¸ä¸€è‡´ï¼‰
        ca_dim = getattr(unet.config, "cross_attention_dim", None)
        assert ca_dim is None or cond_emb.shape[-1] == ca_dim, \
            f"cond_emb dim {cond_emb.shape[-1]} != UNet cross_attention_dim {ca_dim}"

        # 3) éšæœºç§å­
        g = torch.Generator(device=device.type)
        if args.preview_seed is not None:
            g = g.manual_seed(args.preview_seed)

        # 4) ç”Ÿæˆå¹¶ä¿å­˜
        image = pipe(
            prompt_embeds=cond_emb,
            negative_prompt_embeds=neg_emb,
            num_inference_steps=args.preview_steps,
            guidance_scale=args.preview_scale,
            width=args.preview_size,
            height=args.preview_size,
            generator=g
        ).images[0]

        out_dir = os.path.join(args.out_dir, "preview")
        os.makedirs(out_dir, exist_ok=True)
        prefix = "ema" if use_ema else "raw"
        out_path = os.path.join(out_dir, f"step_{step:08d}_{prefix}.png")
        image.save(out_path)
        print(f">> saved preview: {out_path}")

    # ====== æ¢å¤ ======
    start_step, start_epoch, pt_from_state, start_opt_step = try_load_train_state(args.resume, unet, optimizer, lr_sched, ema, scaler)
    if pt_from_state is not None and pt_from_state != prediction_type:
        print(f"[warn] resume checkpoint pred_type={pt_from_state} != current {prediction_type}")

    # ====== æ–‡æœ¬ç¼–ç å™¨ï¼ˆè®­ç»ƒæœŸç°ç®—ï¼‰ ======
    tokenizer = CLIPTokenizer.from_pretrained(CLIP_MODEL)
    text_encoder = CLIPTextModel.from_pretrained(
        CLIP_MODEL, torch_dtype=torch.float16 if device.type=="cuda" else torch.float32
    ).to(device)
    text_encoder.eval().requires_grad_(False)

    @torch.no_grad()
    def encode_texts(text_list: List[str]) -> torch.Tensor:
        ids = tokenizer(text_list, padding="max_length", max_length=MAX_TOKEN,
                        truncation=True, return_tensors="pt").input_ids.to(device)
        with torch.amp.autocast('cuda', enabled=use_amp, dtype=torch.float16):
            return text_encoder(ids)[0]  # [B,77,768]

    # é¢„è®¡ç®— uncond
    uncond_emb = encode_texts([""])  # [1,77,768]

    # æ–‡æœ¬åµŒå…¥ LRU ç¼“å­˜
    CACHE_CAP = getattr(args, "embed_cache_size", 50000)
    cache = OrderedDict()
    def get_emb_cached(s: str) -> torch.Tensor:
        if s in cache:
            e = cache.pop(s); cache[s] = e
            return e
        e = encode_texts([s])  # [1,77,768]
        if len(cache) >= CACHE_CAP:
            cache.popitem(last=False)
        cache[s] = e
        return e

    def sample_variant_index5(mask_i: np.ndarray) -> int:
        p = PROBS_5 * mask_i.astype(np.float64)
        s = p.sum()
        if s <= 0:
            # è¯¥æ ·æœ¬å®Œå…¨æ— æ–‡æœ¬ -> è¿”å›ä»»æ„ç´¢å¼•ï¼Œä½†ç¨åä¼šèµ° uncond
            return int(np.random.choice(5))
        p = p / s
        return int(np.random.choice(5, p=p))

    # ====== è®­ç»ƒå¾ªç¯ ======
    global_step = start_step
    opt_step = start_opt_step
    unet.train()
    optimizer.zero_grad(set_to_none=True)

    for epoch in range(args.epochs):
        pbar = tqdm(loader, desc=f"epoch {epoch+1}/{args.epochs}")
        for batch in pbar:
            # batch: lat [B,4,64,64], cap(list[str])ï¼Œauth(list[str])ï¼Œpreview_text(list[str])
            lat, caps, auths, preview_texts = batch
            lat = lat.to(device, dtype=torch.float16)
            B = lat.size(0)

            # â€”â€” ä¸ºæ¯ä¸ªæ ·æœ¬æŒ‰ 5 æ¡¶æ¦‚ç‡æŒ‘ä¸€æ¡æ–‡æœ¬
            chosen_texts = []
            for i in range(B):
                texts_i, mask_i, _ = _build_variants_from_cap_author(caps[i], auths[i])
                idx = sample_variant_index5(mask_i)
                s = texts_i[idx] if (idx < len(texts_i) and mask_i[idx]) else ""
                print(f"DBG {epoch} chosen idx {idx}: {s}")
                chosen_texts.append(s)

            # â€”â€” æ‰¹é‡/ç¼“å­˜ç¼–ç 
            embs_list = []
            for s in chosen_texts:
                if not s:
                    embs_list.append(uncond_emb.expand(1, -1, -1))
                else:
                    embs_list.append(get_emb_cached(s))
            emb = torch.cat(embs_list, dim=0)  # [B,77,768]

            # â€”â€” CFG dropoutï¼šç­‰ä»· 10% ç©ºæç¤ºï¼ˆå¯é€šè¿‡ --cfg_drop è°ƒæ•´ï¼‰
            if args.cfg_drop > 0.0:
                m = (torch.rand(B, device=device) < args.cfg_drop).view(B, 1, 1)
                emb = torch.where(m, uncond_emb.expand(B, -1, -1), emb)

            # â€”â€” æ‰©æ•£è®­ç»ƒ
            t = torch.randint(0, noise_sched.config.num_train_timesteps, (B,), device=device)
            noise = torch.randn_like(lat)

            with torch.amp.autocast('cuda', enabled=use_amp, dtype=torch.float16):
                noisy = noise_sched.add_noise(lat, noise, t)
                pred = unet(noisy, t, encoder_hidden_states=emb).sample

                target = noise if prediction_type == "epsilon" else noise_sched.get_velocity(lat, noise, t)

                loss = F.mse_loss(pred, target, reduction="none").mean(dim=(1,2,3))
                if args.min_snr_gamma > 0:
                    snr = compute_snr(noise_sched.alphas_cumprod, t)
                    w = min_snr_weights(snr, gamma=args.min_snr_gamma)
                    loss = (loss * w).mean()
                else:
                    loss = loss.mean()

            # â€”â€” è®°å½•/åä¼ /ä¼˜åŒ–
            loss_scalar = float(loss.detach().cpu())
            log_loss(global_step + 1, loss_scalar)
            if (global_step + 1) % plot_interval == 0:
                flush_csv()
                save_plot()

            scaler.scale(loss / args.grad_accum).backward()

            if (global_step + 1) % args.grad_accum == 0:
                nn.utils.clip_grad_norm_(unet.parameters(), 1.0)
                scaler.step(optimizer); scaler.update()
                optimizer.zero_grad(set_to_none=True)
                lr_sched.step()
                ema.update(unet)
                opt_step += 1

            global_step += 1
            pbar.set_postfix({"loss": loss_scalar, "lr": optimizer.param_groups[0]["lr"]})

            # ä¿å­˜ï¼ˆæŒ‰å¾®æ­¥ step å‘½åï¼‰
            if args.save_steps and (global_step % args.save_steps == 0):
                save_ckpt(args, unet, ema, step=global_step, prediction_type=prediction_type)

            # é¢„è§ˆå›¾ï¼ˆç”¨ batch å†…ç¬¬ä¸€æ¡çš„ preview_textï¼‰
            if getattr(args, "preview_every_ckpt", False) and global_step % args.preview_save_steps == 0:
                ptxt = preview_texts[0] if isinstance(preview_texts, list) and len(preview_texts) > 0 else ""
                save_preview_image(ptxt, global_step, use_ema=False)
                save_preview_image(ptxt, global_step, use_ema=True)

        # æ¯ä¸ª epoch ç»“æŸä¹Ÿä¿å­˜ä¸€æ¬¡
        if args.save_epochs and ((epoch + 1) % args.save_epochs == 0):
            save_ckpt(args, unet, ema, step=global_step, prediction_type=prediction_type)

    print(">> training done.")

def save_ckpt(args, unet, ema: EMA, step: int, prediction_type: str):
    # ä¿å­˜å½“å‰ raw æƒé‡
    raw_dir = os.path.join(args.out_dir, f"step_{step}_raw")
    os.makedirs(raw_dir, exist_ok=True)
    torch.save(unet.state_dict(), os.path.join(raw_dir, "unet_raw.pt"))

    # ä¿å­˜ EMA æƒé‡ï¼ˆæ¨èæ¨ç†ç”¨ï¼‰
    ema_model = UNet2DConditionModel.from_config(unet.config).to(unet.device)
    ema.copy_to(ema_model)
    ema_dir = os.path.join(args.out_dir, f"step_{step}_ema")
    os.makedirs(ema_dir, exist_ok=True)

    # ç”¨ safetensors å­˜ä¸€ä»½
    state = {k: v.detach().cpu() for k, v in ema_model.state_dict().items()}
    safetensors_save(state, os.path.join(ema_dir, "unet_ema.safetensors"))

    # ä¿å­˜ config å’Œ meta.json
    with open(os.path.join(ema_dir, "config.json"), "w", encoding="utf-8") as f:
        json.dump(unet.config, f, indent=2)
    with open(os.path.join(ema_dir, "meta.json"), "w", encoding="utf-8") as f:
        json.dump({"prediction_type": prediction_type, "step": step}, f, indent=2)

    print(f">> saved ckpt @ {ema_dir}")

    # â›ï¸ è‡ªåŠ¨æ¸…ç†æ—§çš„ checkpointï¼ˆåªä¿ç•™æœ€è¿‘10ä¸ªï¼‰
    ckpt_dirs = sorted(glob.glob(os.path.join(args.out_dir, "step_*_ema")), key=os.path.getmtime)
    max_keep = 10
    if len(ckpt_dirs) > max_keep:
        for old_dir in ckpt_dirs[:-max_keep]:
            try:
                for file in glob.glob(os.path.join(old_dir, "*")):
                    os.remove(file)
                os.rmdir(old_dir)
                print(f"ğŸ—‘ï¸ deleted old ema ckpt: {old_dir}")
            except Exception as e:
                print(f"âŒ failed to delete {old_dir}: {e}")

        raw_dirs = sorted(glob.glob(os.path.join(args.out_dir, "step_*_raw")), key=os.path.getmtime)
        for old_dir in raw_dirs[:-max_keep]:
            try:
                for file in glob.glob(os.path.join(old_dir, "*")):
                    os.remove(file)
                os.rmdir(old_dir)
                print(f"ğŸ—‘ï¸ deleted old raw ckpt: {old_dir}")
            except Exception as e:
                print(f"âŒ failed to delete {old_dir}: {e}")

# -------------------------
# æ¨ç†ï¼ˆæ›¿æ¢ UNetï¼‰
# -------------------------
def cmd_infer(args):
    from diffusers import StableDiffusionPipeline
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    dtype = torch.float16 if device.type == "cuda" else torch.float32
    pipe = StableDiffusionPipeline.from_pretrained(SD15_REPO, torch_dtype=dtype).to(device)
    pipe.safety_checker = None
    pipe.enable_attention_slicing()
    try:
        pipe.enable_xformers_memory_efficient_attention()
    except Exception:
        pass

    print(">> loading trained UNetâ€¦")
    unet = UNet2DConditionModel.from_config(pipe.unet.config).to(device, dtype=dtype)

    if args.unet_path.endswith(".safetensors"):
        sd = safetensors_load(args.unet_path)
    else:
        sd = torch.load(args.unet_path, map_location="cpu")
    missing, unexpected = unet.load_state_dict(sd, strict=False)
    print("missing:", len(missing), "unexpected:", len(unexpected))
    pipe.unet = unet

    if args.vpred:
        pipe.scheduler.config.prediction_type = "v_prediction"
    else:
        pipe.scheduler.config.prediction_type = "epsilon"

    g = torch.Generator(device=device.type)
    if args.seed is not None:
        g = g.manual_seed(args.seed)

    image = pipe(
        prompt=args.prompt,
        negative_prompt=args.negative_prompt,
        num_inference_steps=args.steps,
        guidance_scale=args.scale,
        width=args.width, height=args.height,
        generator=g
    ).images[0]

    os.makedirs(os.path.dirname(args.out), exist_ok=True)
    image.save(args.out)
    print(">> saved", args.out)

# -------------------------
# CLI
# -------------------------
def build_parser():
    p = argparse.ArgumentParser("SD1.5 UNet Train & Infer (5-way conditioning, runtime text encode)")
    sub = p.add_subparsers(dest="cmd")

    # encode
    pe = sub.add_parser("encode", help="é¢„ç¼–ç å›¾åƒä¸º latentï¼ˆCSV: path,caption,authorï¼‰ï¼Œæ–‡æœ¬ä»…å†™å…¥ index.jsonl")
    pe.add_argument("--csv", type=str, required=True, help="CSV with columns: path,caption,author")
    pe.add_argument("--out_dir", type=str, required=True)
    pe.add_argument("--size", type=int, default=512)
    pe.set_defaults(func=cmd_encode)

    # train
    pt = sub.add_parser("train", help="è®­ç»ƒ UNetï¼ˆ5 æ¡¶é‡‡æ · + è®­ç»ƒæœŸç°ç®—æ–‡æœ¬åµŒå…¥ + CFG dropoutï¼‰")
    pt.add_argument("--data_dir", type=str, required=True, help="encode äº§ç”Ÿçš„æ•°æ®ç›®å½•ï¼ˆå« index.jsonl å’Œ .npzï¼‰")
    pt.add_argument("--out_dir", type=str, required=True)
    pt.add_argument("--batch_size", type=int, default=8)
    pt.add_argument("--workers", type=int, default=4)
    pt.add_argument("--epochs", type=int, default=5)
    pt.add_argument("--lr", type=float, default=2e-4)
    pt.add_argument("--warmup", type=int, default=1000)
    pt.add_argument("--grad_accum", type=int, default=1)
    pt.add_argument("--ema", type=float, default=0.999)
    pt.add_argument("--cfg_drop", type=float, default=0.10, help="æ— æ¡ä»¶ dropout æ¯”ä¾‹ï¼ˆç­‰ä»·åŸ 10% ç©ºæç¤ºï¼‰")
    pt.add_argument("--min_snr_gamma", type=float, default=0.0, help=">0 å¯ç”¨ Min-SNR æƒé‡ï¼ˆå¦‚ 5.0ï¼‰")
    pt.add_argument("--vpred", action="store_true", help="ä½¿ç”¨ v-prediction è®­ç»ƒï¼ˆé»˜è®¤ epsilonï¼‰")
    pt.add_argument("--seed", type=int, default=1234)
    pt.add_argument("--save_steps", type=int, default=2000)
    pt.add_argument("--save_epochs", type=int, default=0, help=">0 åˆ™æ¯ N ä¸ª epoch ä¿å­˜ä¸€æ¬¡")
    pt.add_argument("--embed_cache_size", type=int, default=50000, help="è®­ç»ƒæœŸæ–‡æœ¬åµŒå…¥ LRU ç¼“å­˜ä¸Šé™æ¡ç›®æ•°")

    pt.add_argument("--plot_interval", type=int, default=500, help="æ¯éš”å¤šå°‘ step ä¿å­˜ä¸€æ¬¡æŸå¤±æ›²çº¿")
    pt.add_argument("--loss_ema", type=float, default=0.98, help="loss EMA å¹³æ»‘ç³»æ•°ï¼Œ1.0 è¡¨ç¤ºå…³é—­å¹³æ»‘")

    pt.add_argument("--resume", type=str, default=None, help="ä»ä¿å­˜çš„è®­ç»ƒçŠ¶æ€æ¢å¤ï¼ˆ.pth/.pt æˆ–ç›®å½•ï¼‰")

    # é¢„è§ˆå›¾ç›¸å…³å‚æ•°
    pt.add_argument("--preview_every_ckpt", action="store_true", help="ä¿å­˜æƒé‡æ—¶åŒæ—¶ä¿å­˜é¢„è§ˆå›¾")
    pt.add_argument("--preview_save_steps", type=int, default=2000, help="æ¯ N ä¸ª step ä¿å­˜ä¸€æ¬¡é¢„è§ˆå›¾")
    pt.add_argument("--preview_steps", type=int, default=30, help="é¢„è§ˆå›¾æ¨ç†æ­¥æ•°")
    pt.add_argument("--preview_scale", type=float, default=7.5, help="CFG scale")
    pt.add_argument("--preview_seed", type=int, default=12345, help="é¢„è§ˆå›¾éšæœºç§å­")
    pt.add_argument("--preview_size", type=int, default=512, help="é¢„è§ˆå›¾è¾¹é•¿(æ­£æ–¹å½¢)")
    pt.add_argument("--preview_negative", type=str, default="", help="é¢„è§ˆå›¾çš„è´Ÿé¢æç¤ºè¯")

    pt.set_defaults(func=cmd_train)

    # infer
    pi = sub.add_parser("infer", help="æ›¿æ¢ç®¡çº¿ UNet æ¨ç†å‡ºå›¾")
    pi.add_argument("--unet_path", type=str, required=True, help="è®­ç»ƒäº§å‡ºçš„ EMA/RAW æƒé‡ï¼ˆ.safetensors æˆ– .ptï¼‰")
    pi.add_argument("--prompt", type=str, required=True)
    pi.add_argument("--negative_prompt", type=str, default="")
    pi.add_argument("--steps", type=int, default=30)
    pi.add_argument("--scale", type=float, default=7.5)
    pi.add_argument("--width", type=int, default=512)
    pi.add_argument("--height", type=int, default=512)
    pi.add_argument("--seed", type=int, default=None)
    pi.add_argument("--vpred", action="store_true", help="æ¨ç†ç”¨ v-predictionï¼ˆéœ€ä¸è®­ç»ƒå¯¹é½ï¼‰")
    pi.add_argument("--out", type=str, default="out.png")
    pi.set_defaults(func=cmd_infer)

    # decode
    pd = sub.add_parser("decode", help="å°† latent(.npz) è§£ç ä¸ºå›¾ç‰‡ï¼ˆä½¿ç”¨ SD1.5 çš„ VAEï¼‰")
    pd.add_argument("--input", type=str, required=True, help="å•ä¸ª .npz æ–‡ä»¶æˆ–ç›®å½•")
    pd.add_argument("--out", type=str, default=None, help="å•æ–‡ä»¶æ¨¡å¼çš„è¾“å‡ºè·¯å¾„ï¼ˆ.pngï¼‰")
    pd.add_argument("--out_dir", type=str, default="./decoded", help="ç›®å½•æ¨¡å¼ä¸‹çš„è¾“å‡ºç›®å½•")
    pd.add_argument("--scale", type=float, default=VAE_SCALE, help="latent ç¼©æ”¾ï¼ˆSD1.x=0.18215ï¼‰")
    pd.set_defaults(func=cmd_decode)

    return p

def main():
    parser = build_parser()
    args = parser.parse_args()
    if not hasattr(args, "func"):
        print("Use one of subcommands: encode | train | infer | decode. Example:\n"
              "  python sd15_unet_runtime_text.py encode --csv data.csv --out_dir ./latent_db\n"
              "  python sd15_unet_runtime_text.py train  --data_dir ./latent_db --out_dir ./ckpts --vpred --min_snr_gamma 5 --preview_every_ckpt\n"
              "  python sd15_unet_runtime_text.py infer  --unet_path ./ckpts/step_20000_ema/unet_ema.safetensors "
              "--prompt 'best quality, 1girl, miko' --vpred --out sample.png\n"
              "  python sd15_unet_runtime_text.py decode --input ./latent_db --out_dir ./decoded")
        return
    args.func(args)

if __name__ == "__main__":
    main()
